{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 / 20: \n",
      "----------\n",
      "EPOCH 1 BATCH 1: training batch loss: 5.365843772888184\n",
      "\n",
      ">>>>>>  EPOCH 1 BATCH 1: validation score 0.007007007007007007\n",
      "\n",
      "EPOCH 1 BATCH 6: training batch loss: 5.252135753631592\n",
      "EPOCH 1 BATCH 11: training batch loss: 5.314694404602051\n",
      "\n",
      ">>>>>>  EPOCH 1 BATCH 11: validation score 0.011511511511511512\n",
      "\n",
      "EPOCH 1 BATCH 16: training batch loss: 5.289299488067627\n",
      "EPOCH 1 BATCH 21: training batch loss: 5.2902913093566895\n",
      "\n",
      ">>>>>>  EPOCH 1 BATCH 21: validation score 0.011511511511511512\n",
      "\n",
      "EPOCH 1 BATCH 26: training batch loss: 5.305263996124268\n",
      "EPOCH 1 BATCH 31: training batch loss: 5.3763651847839355\n",
      "\n",
      ">>>>>>  EPOCH 1 BATCH 31: validation score 0.010510510510510511\n",
      "\n",
      "EPOCH 1 BATCH 36: training batch loss: 5.324180603027344\n",
      "EPOCH 1 BATCH 41: training batch loss: 5.269857406616211\n",
      "\n",
      ">>>>>>  EPOCH 1 BATCH 41: validation score 0.012012012012012014\n",
      "\n",
      "EPOCH 1 BATCH 46: training batch loss: 5.292675971984863\n",
      "EPOCH 1 BATCH 51: training batch loss: 5.272904872894287\n",
      "\n",
      ">>>>>>  EPOCH 1 BATCH 51: validation score 0.014014014014014014\n",
      "\n",
      "EPOCH 1 BATCH 56: training batch loss: 5.22144079208374\n",
      "EPOCH 1 BATCH 61: training batch loss: 5.193639755249023\n",
      "\n",
      ">>>>>>  EPOCH 1 BATCH 61: validation score 0.014514514514514515\n",
      "\n",
      "EPOCH 1 BATCH 66: training batch loss: 5.283539295196533\n",
      "EPOCH 1 BATCH 71: training batch loss: 5.139901161193848\n",
      "\n",
      ">>>>>>  EPOCH 1 BATCH 71: validation score 0.014514514514514515\n",
      "\n",
      "EPOCH 1 BATCH 76: training batch loss: 5.252498626708984\n",
      "EPOCH 1 BATCH 81: training batch loss: 5.2677717208862305\n",
      "\n",
      ">>>>>>  EPOCH 1 BATCH 81: validation score 0.02052052052052052\n",
      "\n",
      "EPOCH 1 BATCH 86: training batch loss: 5.180866241455078\n",
      "EPOCH 1 BATCH 91: training batch loss: 5.21657133102417\n",
      "\n",
      ">>>>>>  EPOCH 1 BATCH 91: validation score 0.021021021021021023\n",
      "\n",
      "EPOCH 1 BATCH 96: training batch loss: 5.171990394592285\n",
      "EPOCH 1 BATCH 101: training batch loss: 5.202266216278076\n",
      "\n",
      ">>>>>>  EPOCH 1 BATCH 101: validation score 0.023023023023023025\n",
      "\n",
      "EPOCH 1 BATCH 106: training batch loss: 5.190342903137207\n",
      "EPOCH 1 BATCH 111: training batch loss: 5.176163196563721\n",
      "\n",
      ">>>>>>  EPOCH 1 BATCH 111: validation score 0.02252252252252252\n",
      "\n",
      "EPOCH 1 BATCH 116: training batch loss: 5.197306156158447\n",
      "EPOCH 1 BATCH 121: training batch loss: 5.112648010253906\n",
      "\n",
      ">>>>>>  EPOCH 1 BATCH 121: validation score 0.023523523523523524\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 1/20 [13:51<4:23:11, 831.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1: EPOCH_LOSS: 5.250281436068637\n",
      "EPOCH 2 / 20: \n",
      "----------\n",
      "EPOCH 2 BATCH 1: training batch loss: 5.094849586486816\n",
      "\n",
      ">>>>>>  EPOCH 2 BATCH 1: validation score 0.02702702702702703\n",
      "\n",
      "EPOCH 2 BATCH 6: training batch loss: 5.124922275543213\n",
      "EPOCH 2 BATCH 11: training batch loss: 5.101089954376221\n",
      "\n",
      ">>>>>>  EPOCH 2 BATCH 11: validation score 0.032532532532532535\n",
      "\n",
      "EPOCH 2 BATCH 16: training batch loss: 5.067883014678955\n",
      "EPOCH 2 BATCH 21: training batch loss: 5.103292942047119\n",
      "\n",
      ">>>>>>  EPOCH 2 BATCH 21: validation score 0.035035035035035036\n",
      "\n",
      "EPOCH 2 BATCH 26: training batch loss: 5.0244879722595215\n",
      "EPOCH 2 BATCH 31: training batch loss: 5.048108100891113\n",
      "\n",
      ">>>>>>  EPOCH 2 BATCH 31: validation score 0.03853853853853854\n",
      "\n",
      "EPOCH 2 BATCH 36: training batch loss: 5.034821510314941\n",
      "EPOCH 2 BATCH 41: training batch loss: 5.042508602142334\n",
      "\n",
      ">>>>>>  EPOCH 2 BATCH 41: validation score 0.042042042042042045\n",
      "\n",
      "EPOCH 2 BATCH 46: training batch loss: 5.016838073730469\n",
      "EPOCH 2 BATCH 51: training batch loss: 5.044017314910889\n",
      "\n",
      ">>>>>>  EPOCH 2 BATCH 51: validation score 0.04904904904904905\n",
      "\n",
      "EPOCH 2 BATCH 56: training batch loss: 5.06440544128418\n",
      "EPOCH 2 BATCH 61: training batch loss: 4.987701416015625\n",
      "\n",
      ">>>>>>  EPOCH 2 BATCH 61: validation score 0.05455455455455455\n",
      "\n",
      "EPOCH 2 BATCH 66: training batch loss: 4.976945400238037\n",
      "EPOCH 2 BATCH 71: training batch loss: 4.9990644454956055\n",
      "\n",
      ">>>>>>  EPOCH 2 BATCH 71: validation score 0.06456456456456457\n",
      "\n",
      "EPOCH 2 BATCH 76: training batch loss: 4.9613165855407715\n",
      "EPOCH 2 BATCH 81: training batch loss: 4.881575107574463\n",
      "\n",
      ">>>>>>  EPOCH 2 BATCH 81: validation score 0.07257257257257257\n",
      "\n",
      "EPOCH 2 BATCH 86: training batch loss: 4.956906795501709\n",
      "EPOCH 2 BATCH 91: training batch loss: 4.91493034362793\n",
      "\n",
      ">>>>>>  EPOCH 2 BATCH 91: validation score 0.08708708708708708\n",
      "\n",
      "EPOCH 2 BATCH 96: training batch loss: 4.983653545379639\n",
      "EPOCH 2 BATCH 101: training batch loss: 4.8970112800598145\n",
      "\n",
      ">>>>>>  EPOCH 2 BATCH 101: validation score 0.0940940940940941\n",
      "\n",
      "EPOCH 2 BATCH 106: training batch loss: 4.89906644821167\n",
      "EPOCH 2 BATCH 111: training batch loss: 4.899695873260498\n",
      "\n",
      ">>>>>>  EPOCH 2 BATCH 111: validation score 0.0995995995995996\n",
      "\n",
      "EPOCH 2 BATCH 116: training batch loss: 4.880523204803467\n",
      "EPOCH 2 BATCH 121: training batch loss: 4.870530128479004\n",
      "\n",
      ">>>>>>  EPOCH 2 BATCH 121: validation score 0.1011011011011011\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 2/20 [27:43<4:09:28, 831.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2: EPOCH_LOSS: 4.98726431719653\n",
      "EPOCH 3 / 20: \n",
      "----------\n",
      "EPOCH 3 BATCH 1: training batch loss: 4.755880355834961\n",
      "\n",
      ">>>>>>  EPOCH 3 BATCH 1: validation score 0.1086086086086086\n",
      "\n",
      "EPOCH 3 BATCH 6: training batch loss: 4.793086528778076\n",
      "EPOCH 3 BATCH 11: training batch loss: 4.82103967666626\n",
      "\n",
      ">>>>>>  EPOCH 3 BATCH 11: validation score 0.12012012012012012\n",
      "\n",
      "EPOCH 3 BATCH 16: training batch loss: 4.887209892272949\n",
      "EPOCH 3 BATCH 21: training batch loss: 4.759101867675781\n",
      "\n",
      ">>>>>>  EPOCH 3 BATCH 21: validation score 0.12612612612612611\n",
      "\n",
      "EPOCH 3 BATCH 26: training batch loss: 4.752540588378906\n",
      "EPOCH 3 BATCH 31: training batch loss: 4.70635461807251\n",
      "\n",
      ">>>>>>  EPOCH 3 BATCH 31: validation score 0.13713713713713713\n",
      "\n",
      "EPOCH 3 BATCH 36: training batch loss: 4.755686283111572\n",
      "EPOCH 3 BATCH 41: training batch loss: 4.659982204437256\n",
      "\n",
      ">>>>>>  EPOCH 3 BATCH 41: validation score 0.15015015015015015\n",
      "\n",
      "EPOCH 3 BATCH 46: training batch loss: 4.683686256408691\n",
      "EPOCH 3 BATCH 51: training batch loss: 4.706743240356445\n",
      "\n",
      ">>>>>>  EPOCH 3 BATCH 51: validation score 0.14814814814814814\n",
      "\n",
      "EPOCH 3 BATCH 56: training batch loss: 4.717154502868652\n",
      "EPOCH 3 BATCH 61: training batch loss: 4.584864616394043\n",
      "\n",
      ">>>>>>  EPOCH 3 BATCH 61: validation score 0.14964964964964966\n",
      "\n",
      "EPOCH 3 BATCH 66: training batch loss: 4.542644500732422\n",
      "EPOCH 3 BATCH 71: training batch loss: 4.741243362426758\n",
      "\n",
      ">>>>>>  EPOCH 3 BATCH 71: validation score 0.15265265265265265\n",
      "\n",
      "EPOCH 3 BATCH 76: training batch loss: 4.5825724601745605\n",
      "EPOCH 3 BATCH 81: training batch loss: 4.608870506286621\n",
      "\n",
      ">>>>>>  EPOCH 3 BATCH 81: validation score 0.15965965965965967\n",
      "\n",
      "EPOCH 3 BATCH 86: training batch loss: 4.519789218902588\n",
      "EPOCH 3 BATCH 91: training batch loss: 4.539981365203857\n",
      "\n",
      ">>>>>>  EPOCH 3 BATCH 91: validation score 0.16616616616616617\n",
      "\n",
      "EPOCH 3 BATCH 96: training batch loss: 4.590246677398682\n",
      "EPOCH 3 BATCH 101: training batch loss: 4.600582599639893\n",
      "\n",
      ">>>>>>  EPOCH 3 BATCH 101: validation score 0.17017017017017017\n",
      "\n",
      "EPOCH 3 BATCH 106: training batch loss: 4.5766072273254395\n",
      "EPOCH 3 BATCH 111: training batch loss: 4.558053493499756\n",
      "\n",
      ">>>>>>  EPOCH 3 BATCH 111: validation score 0.17917917917917914\n",
      "\n",
      "EPOCH 3 BATCH 116: training batch loss: 4.475312232971191\n",
      "EPOCH 3 BATCH 121: training batch loss: 4.494813919067383\n",
      "\n",
      ">>>>>>  EPOCH 3 BATCH 121: validation score 0.1856856856856857\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|█▌        | 3/20 [41:35<3:55:37, 831.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 3: EPOCH_LOSS: 4.654855115754946\n",
      "EPOCH 4 / 20: \n",
      "----------\n",
      "EPOCH 4 BATCH 1: training batch loss: 4.519239902496338\n",
      "\n",
      ">>>>>>  EPOCH 4 BATCH 1: validation score 0.1866866866866867\n",
      "\n",
      "EPOCH 4 BATCH 6: training batch loss: 4.541206359863281\n",
      "EPOCH 4 BATCH 11: training batch loss: 4.467905521392822\n",
      "\n",
      ">>>>>>  EPOCH 4 BATCH 11: validation score 0.18918918918918917\n",
      "\n",
      "EPOCH 4 BATCH 16: training batch loss: 4.505221843719482\n",
      "EPOCH 4 BATCH 21: training batch loss: 4.3791728019714355\n",
      "\n",
      ">>>>>>  EPOCH 4 BATCH 21: validation score 0.1991991991991992\n",
      "\n",
      "EPOCH 4 BATCH 26: training batch loss: 4.235015392303467\n",
      "EPOCH 4 BATCH 31: training batch loss: 4.451373100280762\n",
      "\n",
      ">>>>>>  EPOCH 4 BATCH 31: validation score 0.2062062062062062\n",
      "\n",
      "EPOCH 4 BATCH 36: training batch loss: 4.324039936065674\n",
      "EPOCH 4 BATCH 41: training batch loss: 4.2888994216918945\n",
      "\n",
      ">>>>>>  EPOCH 4 BATCH 41: validation score 0.21021021021021022\n",
      "\n",
      "EPOCH 4 BATCH 46: training batch loss: 4.464749336242676\n",
      "EPOCH 4 BATCH 51: training batch loss: 4.286357879638672\n",
      "\n",
      ">>>>>>  EPOCH 4 BATCH 51: validation score 0.2182182182182182\n",
      "\n",
      "EPOCH 4 BATCH 56: training batch loss: 4.17127799987793\n",
      "EPOCH 4 BATCH 61: training batch loss: 4.279941082000732\n",
      "\n",
      ">>>>>>  EPOCH 4 BATCH 61: validation score 0.22922922922922923\n",
      "\n",
      "EPOCH 4 BATCH 66: training batch loss: 4.361850261688232\n",
      "EPOCH 4 BATCH 71: training batch loss: 4.153766632080078\n",
      "\n",
      ">>>>>>  EPOCH 4 BATCH 71: validation score 0.23623623623623624\n",
      "\n",
      "EPOCH 4 BATCH 76: training batch loss: 4.334077835083008\n",
      "EPOCH 4 BATCH 81: training batch loss: 4.335714340209961\n",
      "\n",
      ">>>>>>  EPOCH 4 BATCH 81: validation score 0.24374374374374375\n",
      "\n",
      "EPOCH 4 BATCH 86: training batch loss: 4.300325393676758\n",
      "EPOCH 4 BATCH 91: training batch loss: 4.210538387298584\n",
      "\n",
      ">>>>>>  EPOCH 4 BATCH 91: validation score 0.24824824824824826\n",
      "\n",
      "EPOCH 4 BATCH 96: training batch loss: 4.220678329467773\n",
      "EPOCH 4 BATCH 101: training batch loss: 4.0477776527404785\n",
      "\n",
      ">>>>>>  EPOCH 4 BATCH 101: validation score 0.26226226226226224\n",
      "\n",
      "EPOCH 4 BATCH 106: training batch loss: 4.10352087020874\n",
      "EPOCH 4 BATCH 111: training batch loss: 4.068312644958496\n",
      "\n",
      ">>>>>>  EPOCH 4 BATCH 111: validation score 0.26676676676676675\n",
      "\n",
      "EPOCH 4 BATCH 116: training batch loss: 4.253400802612305\n",
      "EPOCH 4 BATCH 121: training batch loss: 4.107093334197998\n",
      "\n",
      ">>>>>>  EPOCH 4 BATCH 121: validation score 0.26626626626626626\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 4/20 [55:26<3:41:45, 831.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 4: EPOCH_LOSS: 4.288370823597646\n",
      "EPOCH 5 / 20: \n",
      "----------\n",
      "EPOCH 5 BATCH 1: training batch loss: 4.088942050933838\n",
      "\n",
      ">>>>>>  EPOCH 5 BATCH 1: validation score 0.2707707707707708\n",
      "\n",
      "EPOCH 5 BATCH 6: training batch loss: 4.13288688659668\n",
      "EPOCH 5 BATCH 11: training batch loss: 3.9394123554229736\n",
      "\n",
      ">>>>>>  EPOCH 5 BATCH 11: validation score 0.2782782782782783\n",
      "\n",
      "EPOCH 5 BATCH 16: training batch loss: 4.10756254196167\n",
      "EPOCH 5 BATCH 21: training batch loss: 3.9245264530181885\n",
      "\n",
      ">>>>>>  EPOCH 5 BATCH 21: validation score 0.28678678678678676\n",
      "\n",
      "EPOCH 5 BATCH 26: training batch loss: 4.104048252105713\n",
      "EPOCH 5 BATCH 31: training batch loss: 3.8105838298797607\n",
      "\n",
      ">>>>>>  EPOCH 5 BATCH 31: validation score 0.2982982982982983\n",
      "\n",
      "EPOCH 5 BATCH 36: training batch loss: 3.9696292877197266\n",
      "EPOCH 5 BATCH 41: training batch loss: 3.917628526687622\n",
      "\n",
      ">>>>>>  EPOCH 5 BATCH 41: validation score 0.2957957957957958\n",
      "\n",
      "EPOCH 5 BATCH 46: training batch loss: 3.764296054840088\n",
      "EPOCH 5 BATCH 51: training batch loss: 4.115060806274414\n",
      "\n",
      ">>>>>>  EPOCH 5 BATCH 51: validation score 0.3008008008008008\n",
      "\n",
      "EPOCH 5 BATCH 56: training batch loss: 3.9569835662841797\n",
      "EPOCH 5 BATCH 61: training batch loss: 3.8431451320648193\n",
      "\n",
      ">>>>>>  EPOCH 5 BATCH 61: validation score 0.2962962962962963\n",
      "\n",
      "EPOCH 5 BATCH 66: training batch loss: 3.958240509033203\n",
      "EPOCH 5 BATCH 71: training batch loss: 3.9665300846099854\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "# fix seeds\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 2019\n",
    "seed_everything(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.enabled = False\n",
    "print(device)\n",
    "\n",
    "df_class = pd.read_csv('../data/class.csv')\n",
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_train = df_train[['img_file', 'class']]\n",
    "df_train.replace(196, 0, inplace=True)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_train['img_file'], df_train['class'], stratify=df_train['class'], test_size=0.2, random_state=SEED)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_val = X_val.values\n",
    "y_train = y_train.values\n",
    "y_val = y_val.values\n",
    "\n",
    "TRAIN_DATA_PATH = '../data/train_crop/'\n",
    "TEST_DATA_PATH = '../data/test_crop/'\n",
    "\n",
    "class TrainImages(Dataset):\n",
    "    def __init__(self, images, labels, mode=None, transforms=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms[self.mode]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(TRAIN_DATA_PATH + self.images[idx]).convert(\"RGB\")\n",
    "        image = self.transforms(image)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    \n",
    "class TestImages(Dataset):\n",
    "    def __init__(self, images, labels, mode=None, transforms=None):\n",
    "        self.images = images\n",
    "        self.laels = labels\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms[self.mode]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(TEST_DATA_PATH + self.images[idx]).convert(\"RGB\")\n",
    "        image = self.transforms(image)\n",
    "        labels = self.labels[idx]\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.485, 0.456, 0.406],\n",
    "            [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.485, 0.456, 0.406],\n",
    "            [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.485, 0.456, 0.406],\n",
    "            [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TrainImages(images=X_train, labels=y_train, mode='train', transforms=transform)\n",
    "val_dataset = TrainImages(images=X_val, labels=y_val, mode='val', transforms=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train': len(train_dataset),\n",
    "    'val': len(val_dataset)\n",
    "}\n",
    "\n",
    "model_res = models.resnet101(pretrained=True, progress=False)\n",
    "num_features = model_res.fc.in_features\n",
    "model_res.fc = nn.Linear(num_features, 196)\n",
    "\n",
    "optimizer = optim.Adam(model_res.parameters(), lr=0.00001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, dataloaders, dataset_sizes, criterion, optimizer, device, PATH, epochs=20):\n",
    "    start = time.time()\n",
    "\n",
    "    num_classes = 196\n",
    "\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(\"EPOCH {} / {}: \".format(epoch+1, epochs))\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        phase = 'train'\n",
    "\n",
    "        for batch_index, (batch_inputs, batch_labels) in enumerate(dataloaders[phase]):\n",
    "            batch_inputs = batch_inputs.cuda()\n",
    "            batch_labels = batch_labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_inputs)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "            batch_loss = criterion(outputs, batch_labels)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += batch_loss.item() * batch_inputs.size(0)\n",
    "            if batch_index % 5 == 0:\n",
    "                print(\"EPOCH {} BATCH {}: training batch loss: {}\".format(epoch+1, batch_index+1, batch_loss.item()))\n",
    "\n",
    "            if batch_index % 10 == 0:\n",
    "                phase = 'val'\n",
    "                val_preds = np.zeros((dataset_sizes['val'], 1))\n",
    "                val_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    for val_batch_index, (val_batch_inputs, val_batch_labels) in enumerate(dataloaders[phase]):\n",
    "\n",
    "                        val_batch_inputs = val_batch_inputs.cuda()\n",
    "                        val_batch_labels = val_batch_labels.cuda()\n",
    "\n",
    "                        val_outputs = model(val_batch_inputs).detach()\n",
    "                        _, val_batch_preds = torch.max(val_outputs, 1)\n",
    "                        val_batch_loss = criterion(val_outputs, val_batch_labels)\n",
    "                        val_preds[val_batch_index * batch_size: (val_batch_index+1) * batch_size] = val_batch_preds.cpu().view(-1, 1).numpy()\n",
    "                        val_loss += val_batch_loss.item() * val_batch_inputs.size(0)\n",
    "\n",
    "                    val_score = f1_score(y_val, val_preds, average='micro')\n",
    "                    print()\n",
    "                    print(\">>>>>>  EPOCH {} BATCH {}: validation score {}\".format(epoch+1, batch_index+1, val_score))\n",
    "                    print()\n",
    "                    if val_score > best_f1:\n",
    "                        best_f1 = val_score\n",
    "                        best_model_weights = copy.deepcopy(model.state_dict())\n",
    "                        torch.save(model.state_dict(), '../{}/best_model_{}_{}.pt'.format(PATH, epoch+1, batch_index+1))\n",
    "\n",
    "                phase = 'train'\n",
    "                model.train()\n",
    "\n",
    "        epoch_loss = epoch_loss / dataset_sizes['train']\n",
    "        print(\"EPOCH {}: EPOCH_LOSS: {}\".format(epoch+1, epoch_loss))\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    print(\"Training COMPLETED: {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
    "    print(\"BEST VALIDATION F1: {:4f}\".format(best_f1))\n",
    "\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    return model\n",
    "\n",
    "model_res.to(device)\n",
    "model_res = train_model(model=model_res, dataloaders=dataloaders, dataset_sizes=dataset_sizes, criterion=criterion, optimizer=optimizer, device=device, epochs=20, PATH='model/ten_crop/rough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_res.to(device)\n",
    "optimizer = optim.Adam(model_res.parameters(), lr=0.000001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model_res = train_model(model=model_res, dataloaders=dataloaders, dataset_sizes=dataset_sizes, criterion=criterion, optimizer=optimizer, device=device, epochs=100, PATH='model/ten_crop/fine_tune')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "# fix seeds\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 2019\n",
    "seed_everything(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.enabled = False\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFOLD\n",
    "- 5 folds for ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/preser9191/.local/lib/python3.6/site-packages/pandas/core/indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_train.replace(196, 0, inplace=True)\n",
    "df_train['fold'] = 999\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=SEED, shuffle=False)\n",
    "for index, (_, val_index) in enumerate(skf.split(df_train['img_file'], df_train['class'])):\n",
    "    df_train['fold'].iloc[val_index] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fold_dataset(df_train, fold):\n",
    "    X_train = df_train[df_train['fold']==fold]['img_file']\n",
    "    X_val = df_train[df_train['fold']!=fold]['img_file']\n",
    "    y_train = df_train[df_train['fold']==fold]['class']\n",
    "    y_val = df_train[df_train['fold']!=fold]['class']\n",
    "    \n",
    "    return X_train.values, X_val.values, y_train.values, y_val.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Image resize\n",
    "image_size = (224, 224)\n",
    "\n",
    "# Datapath\n",
    "TRAIN_DATA_PATH = '../data/train_crop/'\n",
    "TEST_DATA_PATH = '../data/test_crop/'\n",
    "\n",
    "# torch dataset\n",
    "class TrainImages(Dataset):\n",
    "    def __init__(self, images, labels, mode=None, transforms=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms[self.mode]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(TRAIN_DATA_PATH + self.images[idx]).convert(\"RGB\")\n",
    "        image = self.transforms(image)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    \n",
    "class TestImages(Dataset):\n",
    "    def __init__(self, images, mode=None, transforms=None):\n",
    "        self.images = images\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms[self.mode]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(TEST_DATA_PATH + self.images[idx]).convert(\"RGB\")\n",
    "        image = self.transforms(image)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    \n",
    "transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.485, 0.456, 0.406],\n",
    "            [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.485, 0.456, 0.406],\n",
    "            [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.485, 0.456, 0.406],\n",
    "            [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(X_train, X_val, y_train, y_val):\n",
    "    train_dataset = TrainImages(images=X_train, labels=y_train, mode='train', transforms=transform)\n",
    "    val_dataset = TrainImages(images=X_val, labels=y_val, mode='val', transforms=transform)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    dataloaders = {\n",
    "        'train': train_dataloader,\n",
    "        'val': val_dataloader\n",
    "    }\n",
    "\n",
    "    dataset_sizes = {\n",
    "        'train': len(train_dataset),\n",
    "        'val': len(val_dataset)\n",
    "    }\n",
    "    return dataloaders, dataset_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_index, (batch_image, batch_target) in enumerate(dataloader):\n",
    "        batch_image, batch_target = batch_image.to(device), batch_target.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        batch_output = model(batch_image)\n",
    "        loss = criterion(batch_output, batch_target)\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() / len(dataloader)\n",
    "        if batch_index % (len(dataloader)//3) == 0:\n",
    "            print(\". . . batch: {}, batch_loss: {}\".format(batch_index, loss.item()))\n",
    "    return train_loss\n",
    "\n",
    "def validate(model, dataloader, criterion, dataset_sizes):\n",
    "    model.eval()\n",
    "    \n",
    "    val_preds = np.zeros((dataset_sizes['val'], 1))\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (batch_image, batch_target) in enumerate(dataloader):\n",
    "            batch_image, batch_target = batch_image.to(device), batch_target.to(device)\n",
    "            \n",
    "            batch_output = model(batch_image).detach()\n",
    "            _, batch_preds = torch.max(batch_output, 1)\n",
    "            \n",
    "            loss = criterion(batch_output, batch_target)\n",
    "            val_loss += loss.item() / len(dataloader)\n",
    "            \n",
    "            val_preds[batch_index * batch_size: (batch_index+1) * batch_size] = batch_preds.cpu().view(-1, 1).numpy()\n",
    "\n",
    "        val_score = f1_score(y_val, val_preds, average='micro')\n",
    "        \n",
    "    return val_loss, val_score\n",
    "\n",
    "def train_model(model, dataloaders, dataset_sizes, num_epochs, fold):\n",
    "    \n",
    "    best_score = 0.0\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    results = {\n",
    "        'best_epoch': 0,\n",
    "        'best_score': 0,\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"{} / {}\".format(epoch+1, num_epochs))\n",
    "        train_loss = train(model, dataloaders['train'], criterion, optimizer)\n",
    "        val_loss, val_score = validate(model, dataloaders['val'], criterion, dataset_sizes)\n",
    "        \n",
    "        print(\"EPOCH: {}, train_loss: {}, val_loss: {}, val_score: {}\".format(epoch+1, train_loss, val_loss, val_score))\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            torch.save(model.state_dict(), './fold{}/best_model_epoch_{}_score_{}.pt'.format(fold, epoch+1, val_score))\n",
    "            results['best_epoch'] = epoch+1\n",
    "            results['best_score'] = best_score\n",
    "            print(\">>>>>>  EPOCH {}: validation score {}\".format(epoch+1, val_score))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_res = models.resnet101(pretrained=True, progress=False)\n",
    "num_features = model_res.fc.in_features\n",
    "model_res.fc = nn.Linear(num_features, 196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 20\n",
      ". . . batch: 0, batch_loss: 5.295028209686279\n",
      ". . . batch: 11, batch_loss: 5.424709796905518\n",
      ". . . batch: 22, batch_loss: 5.368289470672607\n",
      "EPOCH: 1, train_loss: 5.45355698556611, val_loss: 11.21972910434969, val_score: 0.006570634318928481\n",
      ">>>>>>  EPOCH 1: validation score 0.006570634318928481\n",
      "2 / 20\n",
      ". . . batch: 0, batch_loss: 5.306900978088379\n",
      ". . . batch: 11, batch_loss: 5.251924514770508\n",
      ". . . batch: 22, batch_loss: 5.335948467254639\n",
      "EPOCH: 2, train_loss: 5.290252136461663, val_loss: 7.616377349822752, val_score: 0.009350518069244376\n",
      ">>>>>>  EPOCH 2: validation score 0.009350518069244376\n",
      "3 / 20\n",
      ". . . batch: 0, batch_loss: 5.235325813293457\n",
      ". . . batch: 11, batch_loss: 5.28553581237793\n",
      ". . . batch: 22, batch_loss: 5.262752532958984\n",
      "EPOCH: 3, train_loss: 5.270599220738264, val_loss: 5.299524603351468, val_score: 0.008971443012383118\n",
      "4 / 20\n",
      ". . . batch: 0, batch_loss: 5.224987983703613\n",
      ". . . batch: 11, batch_loss: 5.237860679626465\n",
      ". . . batch: 22, batch_loss: 5.267725944519043\n",
      "EPOCH: 4, train_loss: 5.249739227872905, val_loss: 5.311193166240568, val_score: 0.013014910285569876\n",
      ">>>>>>  EPOCH 4: validation score 0.013014910285569876\n",
      "5 / 20\n",
      ". . . batch: 0, batch_loss: 5.279523849487305\n",
      ". . . batch: 11, batch_loss: 5.143187046051025\n",
      ". . . batch: 22, batch_loss: 5.185183048248291\n",
      "EPOCH: 5, train_loss: 5.176623705661657, val_loss: 5.2193716687540865, val_score: 0.012130401819560273\n",
      "6 / 20\n",
      ". . . batch: 0, batch_loss: 5.067108154296875\n",
      ". . . batch: 11, batch_loss: 5.004061698913574\n",
      ". . . batch: 22, batch_loss: 5.116161823272705\n",
      "EPOCH: 6, train_loss: 5.0800159194252705, val_loss: 5.17998241224597, val_score: 0.01693201920646955\n",
      ">>>>>>  EPOCH 6: validation score 0.01693201920646955\n",
      "7 / 20\n",
      ". . . batch: 0, batch_loss: 4.838602066040039\n",
      ". . . batch: 11, batch_loss: 5.168944358825684\n",
      ". . . batch: 22, batch_loss: 5.045252799987793\n",
      "EPOCH: 7, train_loss: 5.000286940372352, val_loss: 5.114075841442234, val_score: 0.022239070002527162\n",
      ">>>>>>  EPOCH 7: validation score 0.022239070002527162\n",
      "8 / 20\n",
      ". . . batch: 0, batch_loss: 4.912794589996338\n",
      ". . . batch: 11, batch_loss: 4.819356441497803\n",
      ". . . batch: 22, batch_loss: 4.86229944229126\n",
      "EPOCH: 8, train_loss: 4.885985851287842, val_loss: 129.51078230334863, val_score: 0.007834217841799343\n",
      "9 / 20\n",
      ". . . batch: 0, batch_loss: 4.8055830001831055\n",
      ". . . batch: 11, batch_loss: 4.89215612411499\n",
      ". . . batch: 22, batch_loss: 4.544173717498779\n",
      "EPOCH: 9, train_loss: 4.785809892596619, val_loss: 5.282592834964879, val_score: 0.028051554207733132\n",
      ">>>>>>  EPOCH 9: validation score 0.028051554207733132\n",
      "10 / 20\n",
      ". . . batch: 0, batch_loss: 4.574994087219238\n",
      ". . . batch: 11, batch_loss: 4.5038604736328125\n",
      ". . . batch: 22, batch_loss: 4.730020523071289\n",
      "EPOCH: 10, train_loss: 4.657725868803082, val_loss: 5.197426272976783, val_score: 0.026914329037149357\n",
      "11 / 20\n",
      ". . . batch: 0, batch_loss: 4.388277530670166\n",
      ". . . batch: 11, batch_loss: 4.619510173797607\n",
      ". . . batch: 22, batch_loss: 4.729705333709717\n",
      "EPOCH: 11, train_loss: 4.543192444425641, val_loss: 5.234503207668183, val_score: 0.028177912560020217\n",
      ">>>>>>  EPOCH 11: validation score 0.028177912560020217\n",
      "12 / 20\n",
      ". . . batch: 0, batch_loss: 4.284573554992676\n",
      ". . . batch: 11, batch_loss: 4.254459381103516\n",
      ". . . batch: 22, batch_loss: 4.470968246459961\n",
      "EPOCH: 12, train_loss: 4.4227715694543095, val_loss: 5.1907022768451325, val_score: 0.03702299722011625\n",
      ">>>>>>  EPOCH 12: validation score 0.03702299722011625\n",
      "13 / 20\n",
      ". . . batch: 0, batch_loss: 4.134721755981445\n",
      ". . . batch: 11, batch_loss: 4.305588722229004\n",
      ". . . batch: 22, batch_loss: 4.522492408752441\n",
      "EPOCH: 13, train_loss: 4.269762631618616, val_loss: 5.772335483181864, val_score: 0.03335860500379075\n",
      "14 / 20\n",
      ". . . batch: 0, batch_loss: 3.864063262939453\n",
      ". . . batch: 11, batch_loss: 3.8865163326263428\n",
      ". . . batch: 22, batch_loss: 4.0478644371032715\n",
      "EPOCH: 14, train_loss: 4.128909999674018, val_loss: 5.133910052237973, val_score: 0.04852160727824109\n",
      ">>>>>>  EPOCH 14: validation score 0.04852160727824109\n",
      "15 / 20\n",
      ". . . batch: 0, batch_loss: 3.875157594680786\n",
      ". . . batch: 11, batch_loss: 3.6361327171325684\n",
      ". . . batch: 22, batch_loss: 4.112030029296875\n",
      "EPOCH: 15, train_loss: 3.984811175953258, val_loss: 5.54118000691937, val_score: 0.04612079858478645\n",
      "16 / 20\n",
      ". . . batch: 0, batch_loss: 3.700462818145752\n",
      ". . . batch: 11, batch_loss: 3.640263557434082\n",
      ". . . batch: 22, batch_loss: 3.88147234916687\n",
      "EPOCH: 16, train_loss: 3.808764269857695, val_loss: 5.382781117193163, val_score: 0.049658832448824866\n",
      ">>>>>>  EPOCH 16: validation score 0.049658832448824866\n",
      "17 / 20\n",
      ". . . batch: 0, batch_loss: 3.498143196105957\n",
      ". . . batch: 11, batch_loss: 3.535008668899536\n",
      ". . . batch: 22, batch_loss: 3.85653018951416\n",
      "EPOCH: 17, train_loss: 3.672733595876982, val_loss: 5.888161582331504, val_score: 0.045109931766489766\n",
      "18 / 20\n",
      ". . . batch: 0, batch_loss: 3.6468751430511475\n",
      ". . . batch: 11, batch_loss: 3.7981081008911133\n",
      ". . . batch: 22, batch_loss: 3.589801073074341\n",
      "EPOCH: 18, train_loss: 3.4964488275123373, val_loss: 5.356404881323538, val_score: 0.05521859994945666\n",
      ">>>>>>  EPOCH 18: validation score 0.05521859994945666\n",
      "19 / 20\n",
      ". . . batch: 0, batch_loss: 3.2150750160217285\n",
      ". . . batch: 11, batch_loss: 3.507498025894165\n",
      ". . . batch: 22, batch_loss: 3.25398850440979\n",
      "EPOCH: 19, train_loss: 3.3506807558464273, val_loss: 5.651613143182572, val_score: 0.06330553449583018\n",
      ">>>>>>  EPOCH 19: validation score 0.06330553449583018\n",
      "20 / 20\n",
      ". . . batch: 0, batch_loss: 2.87052321434021\n",
      ". . . batch: 11, batch_loss: 2.8234527111053467\n",
      ". . . batch: 22, batch_loss: 2.953799247741699\n",
      "EPOCH: 20, train_loss: 3.103248791261153, val_loss: 5.4719789874169145, val_score: 0.0731614859742229\n",
      ">>>>>>  EPOCH 20: validation score 0.0731614859742229\n",
      "{'best_epoch': 20, 'best_score': 0.0731614859742229}\n"
     ]
    }
   ],
   "source": [
    "# 0번째 fold\n",
    "fold = 0\n",
    "\n",
    "X_train, X_val, y_train, y_val = prepare_fold_dataset(df_train, fold)\n",
    "dataloaders, dataset_sizes = prepare_dataloader(X_train, X_val, y_train, y_val)\n",
    "\n",
    "model_res.to(device)\n",
    "results = train_model(model_res, dataloaders, dataset_sizes, 20, fold)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, lr, dataset_sizes, num_epochs, fold):\n",
    "    \n",
    "    best_score = 0.0\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    results = {\n",
    "        'best_epoch': 0,\n",
    "        'best_score': 0,\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"{} / {}\".format(epoch+1, num_epochs))\n",
    "        train_loss = train(model, dataloaders['train'], criterion, optimizer)\n",
    "        val_loss, val_score = validate(model, dataloaders['val'], criterion, dataset_sizes)\n",
    "        \n",
    "        print(\"EPOCH: {}, train_loss: {}, val_loss: {}, val_score: {}\".format(epoch+1, train_loss, val_loss, val_score))\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            torch.save(model.state_dict(), './fold{}/best_model_epoch_{}_score_{}.pt'.format(fold, epoch+1, val_score))\n",
    "            results['best_epoch'] = epoch+1\n",
    "            results['best_score'] = best_score\n",
    "            print(\">>>>>>  EPOCH {}: validation score {}\".format(epoch+1, val_score))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 5\n",
      ". . . batch: 0, batch_loss: 2.99566650390625\n",
      ". . . batch: 11, batch_loss: 5.184962749481201\n",
      ". . . batch: 22, batch_loss: 5.0889692306518555\n",
      "EPOCH: 1, train_loss: 5.016065828727953, val_loss: 32.93907719273723, val_score: 0.026661612332575184\n",
      ">>>>>>  EPOCH 1: validation score 0.026661612332575184\n",
      "2 / 5\n",
      ". . . batch: 0, batch_loss: 4.537700176239014\n",
      ". . . batch: 11, batch_loss: 4.581735134124756\n",
      ". . . batch: 22, batch_loss: 4.3808417320251465\n",
      "EPOCH: 2, train_loss: 4.518917618375836, val_loss: 6.798739856289278, val_score: 0.017690169320192066\n",
      "3 / 5\n",
      ". . . batch: 0, batch_loss: 4.210494041442871\n",
      ". . . batch: 11, batch_loss: 4.198036193847656\n",
      ". . . batch: 22, batch_loss: 4.2632365226745605\n",
      "EPOCH: 3, train_loss: 4.231621966217504, val_loss: 5.568101552224929, val_score: 0.03727571392469042\n",
      ">>>>>>  EPOCH 3: validation score 0.03727571392469042\n",
      "4 / 5\n",
      ". . . batch: 0, batch_loss: 3.634830951690674\n",
      ". . . batch: 11, batch_loss: 4.08501672744751\n",
      ". . . batch: 22, batch_loss: 3.8468315601348877\n",
      "EPOCH: 4, train_loss: 3.946481198975534, val_loss: 7.647370665304123, val_score: 0.03373768006065201\n",
      "5 / 5\n",
      ". . . batch: 0, batch_loss: 4.043838024139404\n",
      ". . . batch: 11, batch_loss: 3.530641555786133\n",
      ". . . batch: 22, batch_loss: 3.8909621238708496\n",
      "EPOCH: 5, train_loss: 3.789587772253788, val_loss: 5.684882048637639, val_score: 0.050037907505686124\n",
      ">>>>>>  EPOCH 5: validation score 0.050037907505686124\n",
      "{'best_epoch': 5, 'best_score': 0.050037907505686124}\n"
     ]
    }
   ],
   "source": [
    "model_res = models.resnet101(pretrained=True, progress=False)\n",
    "num_features = model_res.fc.in_features\n",
    "model_res.fc = nn.Linear(num_features, 196)\n",
    "model_res.load_state_dict(torch.load('./fold0/best_model_epoch_20_score_0.0731614859742229.pt'))\n",
    "\n",
    "model_res.to(device)\n",
    "results = train_model(model_res, dataloaders, 0.005, dataset_sizes, 5, fold)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 10\n",
      ". . . batch: 0, batch_loss: 3.2694263458251953\n",
      ". . . batch: 11, batch_loss: 4.109826564788818\n",
      ". . . batch: 22, batch_loss: 4.052751541137695\n",
      "EPOCH: 1, train_loss: 3.9985948259180244, val_loss: 6.772104367133112, val_score: 0.05711397523376295\n",
      ">>>>>>  EPOCH 1: validation score 0.05711397523376295\n",
      "2 / 10\n",
      ". . . batch: 0, batch_loss: 3.530360698699951\n",
      ". . . batch: 11, batch_loss: 3.2438414096832275\n",
      ". . . batch: 22, batch_loss: 3.507331132888794\n",
      "EPOCH: 2, train_loss: 3.4877819942705575, val_loss: 10.146080228590193, val_score: 0.03866565579984837\n",
      "3 / 10\n",
      ". . . batch: 0, batch_loss: 3.220940589904785\n",
      ". . . batch: 11, batch_loss: 3.490468740463257\n",
      ". . . batch: 22, batch_loss: 3.633690357208252\n",
      "EPOCH: 3, train_loss: 3.2608604864640665, val_loss: 8.143711017024133, val_score: 0.03348496335607783\n",
      "4 / 10\n",
      ". . . batch: 0, batch_loss: 2.8262038230895996\n",
      ". . . batch: 11, batch_loss: 3.171001672744751\n",
      ". . . batch: 22, batch_loss: 3.3284010887145996\n",
      "EPOCH: 4, train_loss: 3.03754921392961, val_loss: 6.20016469109443, val_score: 0.07556229466767754\n",
      ">>>>>>  EPOCH 4: validation score 0.07556229466767754\n",
      "5 / 10\n",
      ". . . batch: 0, batch_loss: 2.751173496246338\n",
      ". . . batch: 11, batch_loss: 2.60133695602417\n",
      ". . . batch: 22, batch_loss: 2.72497296333313\n",
      "EPOCH: 5, train_loss: 2.8158435026804605, val_loss: 5.69855651932378, val_score: 0.0813747788728835\n",
      ">>>>>>  EPOCH 5: validation score 0.0813747788728835\n",
      "6 / 10\n",
      ". . . batch: 0, batch_loss: 2.946716070175171\n",
      ". . . batch: 11, batch_loss: 2.163616895675659\n",
      ". . . batch: 22, batch_loss: 2.5513675212860107\n",
      "EPOCH: 6, train_loss: 2.6146970589955654, val_loss: 12.95783608959567, val_score: 0.03272681324235532\n",
      "7 / 10\n",
      ". . . batch: 0, batch_loss: 2.151784658432007\n",
      ". . . batch: 11, batch_loss: 2.1224069595336914\n",
      ". . . batch: 22, batch_loss: 2.5230700969696045\n",
      "EPOCH: 7, train_loss: 2.470937577160922, val_loss: 5.856908025280122, val_score: 0.09236795552186\n",
      ">>>>>>  EPOCH 7: validation score 0.09236795552186\n",
      "8 / 10\n",
      ". . . batch: 0, batch_loss: 2.454819440841675\n",
      ". . . batch: 11, batch_loss: 2.40148663520813\n",
      ". . . batch: 22, batch_loss: 2.220913887023926\n",
      "EPOCH: 8, train_loss: 2.3001108422423857, val_loss: 7.72964952838036, val_score: 0.051680566085418246\n",
      "9 / 10\n",
      ". . . batch: 0, batch_loss: 1.612569808959961\n",
      ". . . batch: 11, batch_loss: 2.2576186656951904\n",
      ". . . batch: 22, batch_loss: 2.2950937747955322\n",
      "EPOCH: 9, train_loss: 2.095170111367197, val_loss: 7.284517430490062, val_score: 0.07164518574677786\n",
      "10 / 10\n",
      ". . . batch: 0, batch_loss: 1.7069357633590698\n",
      ". . . batch: 11, batch_loss: 1.813878059387207\n",
      ". . . batch: 22, batch_loss: 1.9504787921905518\n",
      "EPOCH: 10, train_loss: 1.9398693315910571, val_loss: 7.259115861308193, val_score: 0.08971443012383118\n",
      "{'best_epoch': 7, 'best_score': 0.09236795552186}\n"
     ]
    }
   ],
   "source": [
    "model_res = models.resnet101(pretrained=True, progress=False)\n",
    "num_features = model_res.fc.in_features\n",
    "model_res.fc = nn.Linear(num_features, 196)\n",
    "model_res.load_state_dict(torch.load('./fold0/best_model_epoch_5_score_0.050037907505686124.pt'))\n",
    "\n",
    "model_res.to(device)\n",
    "results = train_model(model_res, dataloaders, 0.005, dataset_sizes, 10, fold)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 10\n",
      ". . . batch: 0, batch_loss: 5.308879852294922\n",
      ". . . batch: 11, batch_loss: 6.237888813018799\n",
      ". . . batch: 22, batch_loss: 5.424022197723389\n",
      "EPOCH: 1, train_loss: 6.124887683174826, val_loss: 86282.33870967742, val_score: 0.005812484205205964\n",
      ">>>>>>  EPOCH 1: validation score 0.005812484205205964\n",
      "2 / 10\n",
      ". . . batch: 0, batch_loss: 5.3067240715026855\n",
      ". . . batch: 11, batch_loss: 5.308599472045898\n",
      ". . . batch: 22, batch_loss: 5.280768871307373\n",
      "EPOCH: 2, train_loss: 5.2822365471811, val_loss: 16.54715223850743, val_score: 0.008213292898660601\n",
      ">>>>>>  EPOCH 2: validation score 0.008213292898660601\n",
      "3 / 10\n",
      ". . . batch: 0, batch_loss: 5.270358085632324\n",
      ". . . batch: 11, batch_loss: 5.284801483154297\n",
      ". . . batch: 22, batch_loss: 5.294198989868164\n",
      "EPOCH: 3, train_loss: 5.2784308664726485, val_loss: 5.311260657925761, val_score: 0.007707859489512257\n",
      "4 / 10\n",
      ". . . batch: 0, batch_loss: 5.293835163116455\n",
      ". . . batch: 11, batch_loss: 5.257975101470947\n",
      ". . . batch: 22, batch_loss: 5.257778167724609\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d23845e56eb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-6f4cf2829202>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, lr, dataset_sizes, num_epochs, fold)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} / {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EPOCH: {}, train_loss: {}, val_loss: {}, val_score: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-39a8d0610340>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, dataloader, criterion, dataset_sizes)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mval_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_res = models.resnet101(pretrained=True, progress=False)\n",
    "num_features = model_res.fc.in_features\n",
    "model_res.fc = nn.Linear(num_features, 196)\n",
    "\n",
    "model_res.to(device)\n",
    "results = train_model(model_res, dataloaders, 0.01, dataset_sizes, 10, fold)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
